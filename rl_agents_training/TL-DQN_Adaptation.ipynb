{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602b94a5",
   "metadata": {},
   "source": [
    "# # TL DQN - Adaptation phase\n",
    "* shared network: 32, 32, 15\n",
    "* learner alpha=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e978c",
   "metadata": {},
   "source": [
    "### import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e02c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from lib.envs.slicing_env import SlicingEnvironment\n",
    "from lib.agents import dqn \n",
    "from lib import utils\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d106652",
   "metadata": {},
   "source": [
    "### configure the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18005474",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### configure the simulation ##################### \n",
    "\n",
    "# set a random seed for reproducibility\n",
    "np.random.seed(2023)\n",
    "\n",
    "# number of DRL agent timesteps per episode \n",
    "max_episode_timesteps = 100\n",
    "\n",
    "total_data_episodes = 1\n",
    "\n",
    "# number of DRL agent episodes (for the sake of better results visulization)\n",
    "total_episodes = 50\n",
    "\n",
    "# qlearning or sarsa\n",
    "# agent_name = 'qlearning'\n",
    "agent_name = 'dqn'\n",
    "\n",
    "learning_type = 'accelerated'\n",
    "loaded_learning_type = 'non_accelerated'\n",
    "\n",
    "# sigmoid reward function configurations\n",
    "c1_volte = 0.5\n",
    "c2_volte = 10\n",
    "c1_urllc = 2\n",
    "c2_urllc = 3\n",
    "c1_video = 1\n",
    "c2_video = 7\n",
    "\n",
    "# q-learning agent configurations\n",
    "discount_factor=0.3\n",
    "alpha=0.01\n",
    "epsilon=0.1 \n",
    "epsilon_decay=0.5 \n",
    "decay_steps=10 \n",
    "exploration = 'e_greedy'\n",
    "\n",
    "# slicing configurations\n",
    "# number of users per slice in the following order: VoLTE, Video, URLLC\n",
    "num_users = [int(46/4), int(46/4), int(8/4)]\n",
    "\n",
    "poisson_volte = np.full((1, 200), 1)\n",
    "poisson_video = np.full((1, 200), 1)\n",
    "poisson_urllc = np.full((1, 200), 1)\n",
    "\n",
    "max_num_users = [max(poisson_volte[0]), max(poisson_video[0]), max(poisson_urllc[0])]\n",
    "\n",
    "num_users_poisson = [poisson_video[0], poisson_volte[0], poisson_urllc[0]]\n",
    "\n",
    "max_size_per_tti = 40\n",
    "max_num_packets = 0\n",
    "max_traffic_percentage = 1\n",
    "num_action_lvls = 15\n",
    "num_slices = 3\n",
    "sl_win_size = 40\n",
    "time_quantum = 1\n",
    "max_trans_per_tti = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe99b7",
   "metadata": {},
   "source": [
    "### generate sample traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7855fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = utils.generate_data(max_num_users[0], max_num_users[1], \n",
    "                                 max_num_users[2], sl_win_size*max_episode_timesteps)\n",
    "traffic_df = traffic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6d57e",
   "metadata": {},
   "source": [
    "## Adaptation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83598641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = os.getcwd()+'/saved_models/base/'\n",
    "\n",
    "# train learner agents using the first batch of reward function weights\n",
    "order = 0\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # set the weights of the learner agent's reward function\n",
    "    w_volte = utils.get_reward_weights_acc(i, order)[0]\n",
    "    w_urllc = utils.get_reward_weights_acc(i, order)[1]\n",
    "    w_video = utils.get_reward_weights_acc(i, order)[2]       \n",
    "    \n",
    "    for j in range(0, 16):\n",
    "        # set the weights of the expert agent's reward function\n",
    "        by_w_volte = utils.get_reward_weights(j)[0]\n",
    "        by_w_urllc = utils.get_reward_weights(j)[1]\n",
    "        by_w_video = utils.get_reward_weights(j)[2]\n",
    "        print('i=%d: w_volte %f, w_urllc %f, w_video %f' %(i, w_volte, w_urllc, w_video))\n",
    "        print('j=%d: by_w_volte %f, by_w_urllc %f, by_w_video %f' %(j,by_w_volte, by_w_urllc, by_w_video))\n",
    "        \n",
    "        by_net_params = base_path + 'net.params_' + \\\n",
    "                        str(exploration) +'_'+ \\\n",
    "                        str(loaded_learning_type) + '_' +  \\\n",
    "                        str(agent_name) + '_' + \\\n",
    "                        str(int(by_w_volte*100)) + str(int(by_w_urllc*100)) +  str(int(by_w_video*100))\n",
    "\n",
    "        # initialize the OpenAI gym-compatible environment using the configured simulation parameters\n",
    "        enviro = SlicingEnvironment(traffic_df, max_num_packets, max_size_per_tti, num_action_lvls, \n",
    "                             num_slices, max_episode_timesteps, sl_win_size, time_quantum,total_data_episodes,\n",
    "                             num_users_poisson, max_traffic_percentage, max_trans_per_tti, w_volte, w_urllc,\n",
    "                                w_video, c1_volte, c1_urllc, c1_video, c2_volte, c2_urllc, c2_video)\n",
    "\n",
    "        env = enviro\n",
    "\n",
    "        # start the simulation using a q-learning agent \n",
    "        qnet, stats = dqn.dqn(env,\n",
    "                      num_episodes=total_episodes,\n",
    "                      exploration=exploration,\n",
    "                      gamma=discount_factor,\n",
    "                      lr=alpha,\n",
    "                      epsilon=epsilon,\n",
    "                      epsilon_decay=epsilon_decay,\n",
    "                      decay_steps=decay_steps,\n",
    "                      loaded_qnet=by_net_params)\n",
    "        \n",
    "        # log the trained agents' data\n",
    "        dictionary = {'config': {'generic': {'max_episode_timesteps': max_episode_timesteps, 'total_episodes': total_episodes,\n",
    "                             'agent_name': agent_name, 'max_size_per_tti': max_size_per_tti,\n",
    "                             'max_traffic_percentage': max_traffic_percentage, 'num_action_lvls': num_action_lvls,\n",
    "                             'num_slices': num_slices, 'sl_win_size': sl_win_size, 'max_trans_per_tti': max_trans_per_tti,\n",
    "                             'w_volte': w_volte, 'w_urllc': w_urllc, 'w_video': w_video, 'by_w_volte': by_w_volte, \n",
    "                             'by_w_urllc': by_w_urllc, 'by_w_video': by_w_video,\n",
    "                             'c1_volte': c1_volte,'c2_volte': c2_volte, 'c1_urllc': c1_urllc, 'c2_urllc': c2_urllc,\n",
    "                             'c1_video': c1_video, 'c2_video': c2_video,\n",
    "                             'learning_type': learning_type},\n",
    "                             'agent_specific': {'discount_factor': discount_factor, 'alpha': alpha,\n",
    "                                                'epsilon': epsilon, 'epsilon_decay': epsilon_decay,\n",
    "                                                'decay_steps': decay_steps, 'loaded_qnet': by_net_params}\n",
    "                            },\n",
    "                  'rewards': {'steps': env.step_rewards, 'episodes': list(stats[1])},\n",
    "                  'KPIs': {'delay': env.total_avg_waiting_times,\n",
    "                           'throughput': env.total_throughputs, 'finished_throughput': env.finished_throughputs,\n",
    "                           'remaining_sizes_sum': env.remaining_sizes_sum, 'remaining_sizes': env.remaining_sizes,\n",
    "                           'remaining_times_sum': env.remaining_times_sum, 'remaining_times': env.remaining_times,\n",
    "                           'total_p_numbers': env.total_p_numbers, 'done_p_numbers': env.done_p_numbers\n",
    "                         }}\n",
    "\n",
    "        # save training data to file\n",
    "        path = 'saved_models/accelerated/'\n",
    "        if not os.path.exists(path):\n",
    "            # create a new directory because it does not exist \n",
    "            os.makedirs(path)\n",
    "        file_name = path + str(learning_type) + '_' + str(agent_name) + '_' + \\\n",
    "                    str(int(w_volte*100)) + str(int(w_urllc*100)) + str(int(w_video*100)) + '_by_' + \\\n",
    "                    str(int(by_w_volte*100)) + str(int(by_w_urllc*100)) + str(int(by_w_video*100)) + '_ep.npy'\n",
    "        np.save(file_name, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d84615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc790f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc0365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
